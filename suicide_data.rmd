---
title: "Suicide Data Analysis"
author: "Harsh Gupta"
date: "3/24/2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

## Load Dataset ##
```{r}
suic_dataset <- read.csv('master.csv', strip.white = TRUE)
suic_dataset <- suic_dataset[, -8]
str(suic_dataset)
head(suic_dataset, 10)
```

## Process Data ##
```{r}
sapply(suic_dataset, function(x) all(is.na(x) || is.infinite(x)))
temp <- suic_dataset
new_colnames <- c('country', 'year', 'sex', 'age', 'suicidect', 'pop', 'suiciderate', 'hdiyear', 'gdpyear', 'gdpcapita', 'gen')
colnames(temp) = new_colnames
custom_factor <- function(cols){
  for(i in cols){
    level_c <- levels(temp[[i]])
    label_c <- c()
    for(j in 0:length(level_c)) label_c[j] <- j
    temp[[i]] <<- factor(temp[[i]], labels = label_c, levels = level_c)
  }
}

cols_ <- colnames(temp[, sapply(temp, is.factor)])
cols_ <- cols_[!cols_ %in% c('country', 'gdpcapita', 'age')]
custom_factor(cols_)

agelevels <- levels(temp$age)
agelevels
agelabels <- c('gen_X', 'boomers', 'silent', 'gen_X', 'gen_GI', 'gen_GI')
temp$age <- factor(temp$age, levels = levels(temp$age), labels = c('gen_X', 'boomers', 'silent', 'gen_X', 'gen_GI', 'gen_GI'))

temp$gdpyear <- as.numeric(gsub(",","",levels(suic_dataset$gdp_for_year....), fixed = TRUE))[suic_dataset$gdp_for_year....]
temp$suiciderate <- (temp$suicidect / (temp$pop / 100000))

temp <- temp[, -11]
temp <- temp[!colnames(temp) %in% c('hdiyear')]
summary(temp)
```

## Preparing Data For Model ##
```{r}
library(reshape)
totals <- function(data){
  tl_df <- data.frame(NA, NA, NA, NA)
  names(tl_df) <- c("country","year","total_pop", "total_suicide_ct")
  tl_df <- na.omit(tl_df)  
  for(i in unique(data$country)){
    for(j in unique(data$year)){
      tdf <- data.frame(i, j, sum(data[data$country == i & data$year == j, 'pop']), 
                        sum(data[data$country == i & data$year == j, 'suicidect']))
      names(tdf) <- c("country", "year","total_pop", "total_suicide_ct")
      tl_df <- rbind(tl_df, tdf)    
    }
  }
  country <- unique(tl_df$country)
  tl_df <- reshape(tl_df, direction = "wide", idvar = c("country"), timevar = "year")
  rownames(tl_df) <- unique(data$country)
  return(tl_df[-1])
}

df <- totals(temp)
head(df,1)

total_df <- data.frame(NA, NA, NA, NA)
names(total_df) <- c("total_pop", "total_suicide_ct")
total_df <- na.omit(total_df) 
for(i in unique(temp$country)){
  tdf <- data.frame(sum(temp[temp$country == i, 'pop']) ,sum(temp[temp$country == i, 'suicidect']))
  colnames(tdf) <- c("total_pop", "total_suicide_ct")
  total_df <- rbind(total_df, tdf)
}

rm(tdf)
rm(i)
rm(j)

rownames(total_df) <- unique(temp$country)
df <- cbind(df, total_df)
scaled_df <- data.frame(scale(df))
head(scaled_df,1)
```

## Making Clusters ##
```{r message=FALSE}
library(factoextra)
distance <- get_dist(scaled_df[, 65:66])
fviz_dist(distance, show_labels = TRUE, lab_size = NULL,
          gradient = list(low = "blue", below_average = "yellow", average = "green", high = "red", extremely_high = "black"))
```
---
##Creating Model
Taking 6 clusters as starting point
<br/>
**Animation of steps to classify data in clusters**
```{r warning=FALSE, message=FALSE}
library(animation)
library(heatmap3)

kmeans.ani(scaled_df[, 65:66], centers = 4)

km <- kmeans(scaled_df[, 65:66], centers = 4, nstart = 30)
str(km)
km
fviz_cluster(km, data = scaled_df[, 65:66])

heatmap3(matrix(scaled_df$total_suicide_ct, km$cluster))
```

## Finding optimal number of clusters 
```{r warning=FALSE}
#Elbow Method
set.seed(300)
fviz_nbclust(scaled_df[, 65:66], kmeans, method="wss") + geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")


#Silhouette Method
set.seed(300)
fviz_nbclust(scaled_df[, 65:66], kmeans, method="silhouette") + labs(subtitle = "Silhouette method")

#Gap Method
library(cluster)
gap_stat <- clusGap(scaled_df[, 65:66], FUN = kmeans, nstart = 30, K.max = 6, B = 50)
set.seed(300)
fviz_gap_stat(gap_stat) + labs(subtitle = "Gap-Statisitc method")

library(NbClust)
NbClust(scaled_df[, 65:66], distance = "euclidean", method = "kmeans")
```
So we stick with 4 clusters and hence we have a robust model. Calculating goodness

```{r}
  goodness_km <- km$betweenss / km$totss
  goodness_km
```

## Validating clusters using validation sets ##
```{r warning=FALSE, message=FALSE}
clusters_df <- data.frame(NA, NA)
colnames(clusters_df) <- c("country", "cluster")
clusters_df <- na.omit(clusters_df)
for(i in levels(factor(km$cluster))){
  rnames <- rownames(df[km$cluster == i, ])
  for(j in rnames){
    tempdf <- data.frame(j, i)
    colnames(tempdf) <- c("country", "cluster")
    clusters_df <- rbind(clusters_df, tempdf)
  }
}
rownames(clusters_df) <- clusters_df$country
clusters_df <- clusters_df[-1]

df <- merge(clusters_df,df, by='row.names', all=TRUE)
scaled_df <- merge(clusters_df, scaled_df, by="row.names", all = TRUE)
```
## Train and Test Data ##
```{r warning=FALSE, message=FALSE}
library(caTools)
library(dplyr)
set.seed(123)
n = nrow(scaled_df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.8, 0.2))
training_set <- scaled_df[split, c(1,2,67,68)]
test_set <- scaled_df[!split, c(1,2,67,68)]
str(test_set)
```
##Predicting Clusters for training data##
```{r}
library(clue)
pred <- cl_predict(km, training_set[,3:4])
str(pred)
result_df <- data.frame(cbind(original_cluster = training_set$cluster, 
                   predicted_cluster = pred))
rownames(result_df) <- training_set$Row.names
head(result_df, 10)
correctly_predicted <- nrow(result_df[result_df$original_cluster == result_df$predicted_cluster, ])
accuracy_rate <- correctly_predicted / nrow(result_df)
accuracy_rate
```
